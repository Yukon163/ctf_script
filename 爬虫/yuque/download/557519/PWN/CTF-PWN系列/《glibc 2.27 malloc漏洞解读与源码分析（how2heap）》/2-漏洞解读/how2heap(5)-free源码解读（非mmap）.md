+ ä¸»è¦é’ˆå¯¹ç‚¹ï¼šå•çº¿ç¨‹

# __libc_freeæºç 
```c
void
__libc_free (void *mem) //ä¼ å…¥è¦é‡Šæ”¾å †å—çš„æŒ‡é’ˆ
{
  mstate ar_ptr;
  mchunkptr p;                          /* chunk corresponding to mem */

  void (*hook) (void *, const void *)  
    = atomic_forced_read (__free_hook);
  if (__builtin_expect (hook != NULL, 0))
    {
      (*hook)(mem, RETURN_ADDRESS (0));
      return;
    } //ä»¥ä¸Šéƒ½æ˜¯å…³äº__free_hookçš„ä»£ç 

  if (mem == 0)  //free(0)ä¼šç›´æ¥åœ¨è¿™é‡Œè¿”å›                 
    return;

  p = mem2chunk (mem);

  if (chunk_is_mmapped (p))                       /* release mmapped memory. */
    {
		//çœç•¥é‡Šæ”¾ç”±mmapåˆ†é…å †å—çš„ä»£ç ......
    }

  MAYBE_INIT_TCACHE (); //æ£€æŸ¥æ˜¯å¦åˆå§‹åŒ–äº†tcacheï¼Œå¦‚æœæ²¡æœ‰åˆå§‹åŒ–åˆ™åˆå§‹åŒ–

  ar_ptr = arena_for_chunk (p);  //è·å–å¯¹åº”çš„arenaåœ°å€ï¼Œåœ¨å•çº¿ç¨‹ä¸­ä¸ºmain_arena
  _int_free (ar_ptr, p, 0); //è°ƒç”¨_int_free
}
libc_hidden_def (__libc_free)
```

# _int_freeæºç 
é¦–å…ˆè¯´æ˜ï¼š

```c
pwndbg> x/16gx 0x555555756250
0x555555756250:	0x0000000000000000	0x0000000000000021 #å‰ä¸€ä¸ªå †å—ï¼šchunk    ï¼ˆå‰ï¼‰
0x555555756260:	0x0000000000000000	0x0000555555756010
0x555555756270:	0x0000000000000000	0x0000000000000031 #p
0x555555756280:	0x0000000000000000	0x0000000000000000
0x555555756290:	0x0000000000000000	0x0000000000000000
0x5555557562a0:	0x0000000000000000	0x0000000000000041 #åä¸€ä¸ªå †å—ï¼šnextchunkï¼ˆåï¼‰
0x5555557562b0:	0x0000000000000000	0x0000000000000000
0x5555557562c0:	0x0000000000000000	0x0000000000000000
pwndbg> 
```

```c
/*
   ------------------------------ free ------------------------------
 */

static void
_int_free (mstate av, mchunkptr p, int have_lock) 
{ //æ­¤å‡½æ•°ä¼ å…¥çš„å‚æ•°åˆ†åˆ«ä¸ºmain_arenaï¼Œå¾…é‡Šæ”¾å †å—çš„æŒ‡é’ˆï¼Œå’Œé”have_lock
  INTERNAL_SIZE_T size;        /* its size */
  mfastbinptr *fb;             /* associated fastbin */
  mchunkptr nextchunk;         /* next contiguous chunk */
  INTERNAL_SIZE_T nextsize;    /* its size */
  int nextinuse;               /* true if nextchunk is used */
  INTERNAL_SIZE_T prevsize;    /* size of previous contiguous chunk */
  mchunkptr bck;               /* misc temp for linking */
  mchunkptr fwd;               /* misc temp for linking */

  size = chunksize (p); //è·å–å¾…é‡Šæ”¾å †å—çš„å¤§å°

  /* Little security check which won't hurt performance: the
     allocator never wrapps around at the end of the address space.
     Therefore we can exclude some size values which might appear
     here by accident or by "design" from some intruder.  */
  if (__builtin_expect ((uintptr_t) p > (uintptr_t) -size, 0) //æ£€æŸ¥å †å—æŒ‡é’ˆçš„æœ‰æ•ˆæ€§
      || __builtin_expect (misaligned_chunk (p), 0))
    malloc_printerr ("free(): invalid pointer");
  /* We know that each chunk is at least MINSIZE bytes in size or a
     multiple of MALLOC_ALIGNMENT.  */
  if (__glibc_unlikely (size < MINSIZE || !aligned_OK (size))) //æ£€æŸ¥å †å—çš„å¤§å°æ˜¯å¦åˆæ³•
    malloc_printerr ("free(): invalid size");

  check_inuse_chunk(av, p);

#if USE_TCACHE
 	//å…³äºtcachebinçš„ä»£ç åœ¨è¿™é‡Œçœç•¥ï¼Œå› ä¸ºä¹‹å‰è¯´è¿‡......
#endif

  /*
    If eligible, place chunk on a fastbin so it can be found
    and used quickly in malloc.
  */

  if ((unsigned long)(size) <= (unsigned long)(get_max_fast ())
    	//æœ‰å…³äºfastbinçš„ä»£ç å·²çœç•¥ï¼Œå› ä¸ºä¹‹å‰ä¹Ÿè¯´è¿‡......

  /*
    Consolidate other non-mmapped chunks as they arrive.
  */
  #æ¥ä¸‹æ¥çš„ä»£ç æ‰æ˜¯é‡å¤´æˆ
  else if (!chunk_is_mmapped(p)) {
	 //è‹¥å¾…é‡Šæ”¾çš„chunkä¸ç”±mmapè¿›è¡Œåˆ†é…
    /* If we're single-threaded, don't lock the arena.  */
    //å¦‚æœæ­¤ç¨‹åºæ˜¯å•çº¿ç¨‹ï¼Œä¸ä¼šå¯¹main_arenaåŠ é”
    if (SINGLE_THREAD_P) 
      have_lock = true; //å•çº¿ç¨‹æ‰§è¡Œï¼šhave_lock=true

    if (!have_lock)
      __libc_lock_lock (av->mutex); //å¤šçº¿ç¨‹è·å¾—é”

    nextchunk = chunk_at_offset(p, size); //è·å–p chunkç›¸é‚»é«˜åœ°å€çš„chunkåœ°å€

    /* Lightweight tests: check whether the block is already the
       top block.  */
    if (__glibc_unlikely (p == av->top))   //å®‰å…¨æ€§æ£€æŸ¥ï¼špä¸èƒ½ä¸ºtop_chunk
      malloc_printerr ("double free or corruption (top)");
    /* Or whether the next chunk is beyond the boundaries of the arena.  */
    if (__builtin_expect (contiguous (av) 
			  && (char *) nextchunk //å®‰å…¨æ€§æ£€æŸ¥ï¼šå½“å‰freeçš„chunkçš„ç›¸é‚»ä¸‹ä¸€ä¸ªchunkä¸èƒ½è¶…è¿‡arenaçš„è¾¹ç•Œ
			  >= ((char *) av->top + chunksize(av->top)), 0))
	malloc_printerr ("double free or corruption (out)");
    /* Or whether the block is actually not marked used.  */
    if (__glibc_unlikely (!prev_inuse(nextchunk))) //è·å–nextchunkçš„PREV_INUSEæ ‡å¿—ä½
        						//å¦‚æœæ ‡å¿—ä½ä¸º0åˆ™è¯´æ˜på †å—å¤„äºfreeçŠ¶æ€ï¼Œä¼šè§¦å‘å¼‚å¸¸
        						//æˆ–è€…è¯´æ˜æˆ‘ä»¬ä¼ªé€ çš„å †å—ä¸æˆåŠŸï¼ˆğŸ˜ï¼‰
      malloc_printerr ("double free or corruption (!prev)");

    nextsize = chunksize(nextchunk); //è·å–nextchunkçš„å¤§å°ï¼ˆä¸åŒ…æ‹¬ä¸‰ä¸ªæ ‡å¿—ä½ï¼‰
    if (__builtin_expect (chunksize_nomask (nextchunk) <= 2 * SIZE_SZ, 0)
	|| __builtin_expect (nextsize >= av->system_mem, 0)) //æ£€æŸ¥nextchunkçš„å¤§å°æ˜¯å¦æ­£å¸¸
      malloc_printerr ("free(): invalid next size (normal)");

    free_perturb (chunk2mem(p), size - 2 * SIZE_SZ); //é»˜è®¤æ¸…ç©ºå †å—ä¸­user_dataçš„å†…å®¹
      //#define chunk2mem(p)   ((void*)((char*)(p) + 2*SIZE_SZ))
    /* consolidate backward */ //å‘ååˆå¹¶
    if (!prev_inuse(p)) { //è·å–pchunkçš„PREV_INUSEæ ‡å¿—ä½
        //å¦‚æœæ ‡å¿—ä½ä¸º0åˆ™è¯´æ˜pä¹‹å‰çš„å †å—å¤„äºfreeçŠ¶æ€ï¼ˆtcachebinã€fastbiné™¤å¤–ï¼‰
        //æˆ‘ä»¬å¼€å§‹å¯¹på’Œå‰ä¸€ä¸ªç›¸é‚»çš„free chunkè¿›è¡Œåˆå¹¶
      prevsize = prev_size (p); //è·å–på †å—çš„mchunk_prev_size
        //å¦‚æœå‰ä¸€ä¸ªå †å—å¤„äºunsortedbinã€smallbinã€largebinåˆ™pçš„mchunk_prev_sizeç½®ä½
        //å’Œä¹‹å‰çš„if (!prev_inuse(p))ç›¸ç…§åº”
        //è¿™é‡Œçš„prevsizeæŒ‡çš„æ˜¯å‰ä¸€ä¸ªå †å—çš„å¤§å°
      size += prevsize; //size=size+prevsize
      p = chunk_at_offset(p, -((long) prevsize));  //pç°åœ¨æŒ‡å‘å‰ä¸€ä¸ªå †å—
      unlink(av, p, bck, fwd); //å¯¹å‰ä¸€ä¸ªå †å—è¿›è¡Œunlink
    }

    if (nextchunk != av->top) { //æ£€æŸ¥på †å—æ˜¯å¦å’Œtop_chunkç›¸é‚»
      /* get and clear inuse bit */ //å¦‚æœä¸å’Œtop_chunkç›¸é‚»
      nextinuse = inuse_bit_at_offset(nextchunk, nextsize); 
        	//è·å–nextchunkçš„ä¸‹ä¸€ä¸ªç´§é‚»çš„å †å—çš„PREV_INUSEæ ‡å¿—ä½
        	//è¿™ä¸ªæ ‡å¿—ä½ä»£è¡¨ç€nextchunkçš„çŠ¶æ€

      /* consolidate forward */ //å‘å‰åˆå¹¶
      if (!nextinuse) { //å¦‚æœnextchunkå¤„äºunsortedbinã€smallbinã€largebinä¸­
	unlink(av, nextchunk, bck, fwd); //å¯¹nextchunkè¿›è¡Œunlink
	size += nextsize; //size=size+nextsize
      } else
	clear_inuse_bit_at_offset(nextchunk, 0); //å°†nextchunkçš„PREV_INUSEæ ‡å¿—ä½ç½®0

      /*
	Place the chunk in unsorted chunk list. Chunks are
	not placed into regular bins until after they have
	been given one chance to be used in malloc.
      */ //å°†å †å—æ”¾å…¥åˆ°unsortedbiné“¾è¡¨ä¸­ï¼Œæˆ‘ä»¬ä¼šåœ¨è°ƒç”¨mallocå‡½æ•°æ—¶å¯¹å…¶ä¸­çš„free chunkè¿›è¡Œæ•´ç†

      bck = unsorted_chunks(av);  //bckæŒ‡å‘unsortedbinçš„main_arena
        //ä¸ºäº†å½¢å®¹æ–¹ä¾¿ï¼Œæˆ‘ä»¬ä½¿ç”¨å·¦å³è¿™ä¸ªæ¦‚å¿µæ¥åŒºåˆ†
        //unsortedbin
        //all: 0x555555757760 â€”â–¸ 0x555555758100 â€”â–¸ 0x7ffff7dcdca0 (main_arena+96) â—‚â€” 0x555555757760 /* '`wuUUU' */
        //           å·¦                  å³
      fwd = bck->fd;  //fwdæŒ‡å‘æœ€å·¦è¾¹çš„å †å— 
      if (__glibc_unlikely (fwd->bk != bck)) //æ£€æŸ¥åŒå‘é“¾è¡¨çš„å®Œæ•´æ€§
	malloc_printerr ("free(): corrupted unsorted chunks");
      p->fd = fwd; //è®¾ç½®på †å—çš„fd
      p->bk = bck; //è®¾ç½®på †å—çš„bk
      if (!in_smallbin_range(size))
	{
	  p->fd_nextsize = NULL;
	  p->bk_nextsize = NULL;
	}
      bck->fd = p; //é“¾å…¥unsortedbinã€1ã€‘:åœ¨æœ€å·¦è¾¹æ’å…¥
      fwd->bk = p; //é“¾å…¥unsortedbinã€2ã€‘

      set_head(p, size | PREV_INUSE); //è®¾ç½®pçš„size
      set_foot(p, size); //è®¾ç½®ç›¸é‚»ä¸‹ä¸€ä¸ªå †å—çš„mchunk_prev_size

      check_free_chunk(av, p); 
    }

    /*
      If the chunk borders the current high end of memory,
      consolidate into top
    */

    else { //å¦‚æœpå †å—å’Œtop_chunkç›¸é‚»
      size += nextsize; //æ­¥éª¤å¾ˆç®€å•ï¼Œä¸top_chunkè¿›è¡Œåˆå¹¶
      set_head(p, size | PREV_INUSE);
      av->top = p;
      check_chunk(av, p);
    }

    /*
      If freeing a large space, consolidate possibly-surrounding
      chunks. Then, if the total unused topmost memory exceeds trim
      threshold, ask malloc_trim to reduce top.

      Unless max_fast is 0, we don't know if there are fastbins
      bordering top, so we cannot tell for sure whether threshold
      has been reached unless fastbins are consolidated.  But we
      don't want to consolidate on each free.  As a compromise,
      consolidation is performed if FASTBIN_CONSOLIDATION_THRESHOLD
      is reached.
    */
    //ç°åœ¨p chunkå·²ç»å’Œå…¶ä»–å †å—åˆå¹¶å®Œæˆ
    if ((unsigned long)(size) >= FASTBIN_CONSOLIDATION_THRESHOLD) {
        //å¦‚æœåˆå¹¶ä¹‹åçš„å †å—å¤§å°å¤§äºFASTBIN_CONSOLIDATION_THRESHOLD(65536UL)
        //ä¸€èˆ¬åˆå¹¶åˆ°top chunkéƒ½ä¼šæ‰§è¡Œè¿™éƒ¨åˆ†ä»£ç ã€‚
      if (atomic_load_relaxed (&av->have_fastchunks)) //åˆ¤æ–­fastbinä¸­æ˜¯å¦æœ‰free chunk
	malloc_consolidate(av); //è°ƒç”¨malloc_consolidateå¯¹fastbin free chunkè¿›è¡Œåˆå¹¶

      if (av == &main_arena) { //åœ¨å•çº¿ç¨‹ä¸­avæ€»æ˜¯æŒ‡å‘main_arena
#ifndef MORECORE_CANNOT_TRIM
          //ä¸»åˆ†é…åŒºï¼Œå¦‚æœå½“å‰top_chunkå¤§å°å¤§äºheapçš„æ”¶ç¼©é˜ˆå€¼ï¼Œå°±ä¼šè°ƒç”¨systrimæ”¶ç¼©heap
	if ((unsigned long)(chunksize(av->top)) >=
	    (unsigned long)(mp_.trim_threshold))
	  systrim(mp_.top_pad, av);
#endif
      } else { //éä¸»åˆ†é…åŒºï¼Œæ”¶ç¼©è°ƒç”¨heap_trimæ”¶ç¼©heapæ®µ
	/* Always try heap_trim(), even if the top chunk is not
	   large, because the corresponding heap might go away.  */
	heap_info *heap = heap_for_ptr(top(av));

	assert(heap->ar_ptr == av);
	heap_trim(heap, mp_.top_pad);
      }
    }

    if (!have_lock)
      __libc_lock_unlock (av->mutex);
  }
  /*
    If the chunk was allocated via mmap, release via munmap().
  */

  else { //å¦‚æœfree chunkç”±mmapè¿›è¡Œåˆ†é…
    munmap_chunk (p);//è°ƒç”¨munmap_chunkç”¨æ¥å›æ”¶mmapåˆ†é…çš„ç©ºé—´ï¼ˆå †å—ï¼‰ã€‚
  }
}
```

